{
  
    
        "post0": {
            "title": "Deep Learning & Applied AI",
            "content": "Welcome to the Deep Learning and Applied AI lab sessions! . During the lab sessions, you will be guided through one or more Python notebooks that teach you deep learning tools and provide opportunities to apply what you have learned in class. . We encourage you to form small groups of 2-3 people to read and discuss the notebooks together. . Run the code and play with it! It is very easy to edit the code locally and make small experiments. Try whatever comes to your mind, this is the best way to learn! Python notebooks are designed to be used in this way, that&#39;s why we chose them for the DLAI lab sessions. . There will be some exercises, try to do them by yourself, and when everyone in your group has finished, compare the solutions with each other. . When something is not clear or your group have a question, raise your hand and we will come to you. . Some sections in the notebooks are marked with 📖. This is deepening content for further reading outside of class. You may want to go through it at home or during class if you finish early. (Some sections are more optional than others, those are marked with more books 📖📖) . Let&#39;s start! . Introduction . Many Deep Learning frameworks have emerged for python. Arguably the most notable ones in 2023 are PyTorch, TensorFlow (with keras frontend) and Jax. We will use PyTorch, which is the leading DL framework for research and continues to gain popularity. . The fundamental data structure of these frameworks is the tensor, which is more or less the same everywhere. Gaining a solid understanding of how tensors work is required in deep learning and will definitely come in handy in other areas. . The first two tutorials will give you solid basics of tensors and operations between tensors. . Wait, wait, wait... what is this strange web page with code and text cells all around? . It is called Colab, an environment to play with python notebooks directly in your web browser, made by Google. If you never used Colab before, take a look to the following cells, adapted from the official Colab guide. . Getting started with Colab . Colab, or &quot;Colaboratory&quot;, allows you to write and execute Python in your browser, with . Zero configuration required | Access to GPUs free of charge | Easy sharing | . The document you are reading is not a static web page, but an interactive environment called a Colab notebook that lets you write and execute code. . For example, here is a code cell with a short Python script that computes a value, stores it in a variable, and prints the result: . seconds_in_a_day = 24 * 60 * 60 seconds_in_a_day . 86400 . To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut &quot;Command/Ctrl+Enter&quot;. To edit the code, just click the cell and start editing. . Variables that you define in one cell can later be used in other cells: . seconds_in_a_week = 7 * seconds_in_a_day seconds_in_a_week . 604800 . Colab notebooks allow you to combine executable code and rich text in a single document, along with images, HTML, LaTeX and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see Overview of Colab. To create a new Colab notebook you can use the File menu above, or use the following link: create a new Colab notebook. . Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see jupyter.org. . Numpy . The adoptive father of python&#39;s deep learning frameworks is Numpy, the historical library which added support for large, multi-dimensional arrays and matrices to Python. . As we will see, modern deep learning frameworks and especially PyTorch have drawn largely from Numpy&#39;s API, while at the same time overcoming its limitations, especially the absence of GPU support or automatic differentiation. The student has become the master. . . We will mainly use PyTorch tensors for implementing our Deep Learning systems, but knowing how to use Numpy remains very important. Note that: . Numpy arrays and PyTorch tensors are very similar, most of the features that we will explain for PyTorch tensors apply also to Numpy arrays. | In real DL systems you need to constantly switch between PyTorch and Numpy. | . If you have previous knowledge in Matlab, we recommend the numpy for Matlab users page as a useful resource. . import numpy as np . PyTorch . During the course we&#39;ll use and learn many parts of PyTorch API. You should also familiarize with the PyTorch Documentation as it will greatly assist you. . import torch torch.__version__ . &#39;1.13.1+cu116&#39; . PyTorch Tensor . The Tensor class is very similar to numpy&#39;s ndarray and provides most of its functionality. . However, it also has two important distinctions: . Support for GPU computations. | Each tensor may store extra information needed to perform back propagation: The gradient tensor w.r.t. some variable (e.g. the loss) | A node representing an operation in the computational graph that produced this tensor. | . | . Keep in mind: . Usually tensor operations are not in-place | . Tensor instantiation . A tensor represents an n-dimensional grid of values, all of the same type. . torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32) . tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32) . torch.zeros((3,5)) . tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . torch.ones((2,5), dtype=torch.float64) . tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]], dtype=torch.float64) . torch.eye(4) . tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) . torch.rand((2,2)) # from which distribution these random numbers are sampled? Check the PyTorch documentation . tensor([[0.9936, 0.7598], [0.2800, 0.2863]]) . torch.randint(0, 100, (3,3)) . tensor([[97, 4, 0], [95, 38, 52], [53, 59, 91]]) . t = torch.rand((3, 3)) torch.ones_like(t) . tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) . It is possible to easily convert to/from Numpy tensors: . t = torch.rand((3, 3), dtype=torch.float32) t.numpy() . array([[0.18455875, 0.9731517 , 0.21335477], [0.2962724 , 0.4785337 , 0.0848493 ], [0.23716253, 0.69917524, 0.7276147 ]], dtype=float32) . n = np.random.rand(3,3).astype(np.float16) torch.from_numpy(n) . tensor([[0.8750, 0.9087, 0.2207], [0.8774, 0.2549, 0.8057], [0.7993, 0.4502, 0.2627]], dtype=torch.float16) . There are many other functions available to create tensors! . EXERCISE . Create a matrix $M in mathbb{R}^{3 times 3}$ that is filled with 2 along the diagonal and 1 elsewhere, that is:&gt;&gt; $$ m_{ij} = begin{cases} 2 &amp; text{if } i = j 1 &amp; text{otherwise} end{cases} $$ . . torch.ones((3,3)) + torch.eye(3) . tensor([[2., 1., 1.], [1., 2., 1.], [1., 1., 2.]]) . Tensor properties . The type of a tensor is the type of each element contained in the tensor: . t = torch.rand((3, 3)) t.dtype . torch.float32 . The shape of a tensor is a tuple of integers giving the size of the tensor along each dimension, e.g. for a matrix $M in mathbb{R}^{3 times 5}$: . t = torch.rand((3,5)) t.shape . torch.Size([3, 5]) . The device of a tensor indicates the memory in which the tensor currently resides: RAM (denoted as cpu) or GPU memory (denoted as cuda) . t = torch.rand((3,5)) t.device . device(type=&#39;cpu&#39;) . EXERCISE . Given a matrix $X in mathbb{R}^{m times n}$, create another matrix $Y in mathbb{R}^{m times 3}$ filled with ones using $X$. . x = torch.rand(100,42) # Your solution: # y = ? . torch.ones((x.shape[0], 3)) . Using the GPU . Thanks to the explosion of the videogame industry in the last 50 years, the performance of the chips specialized in rendering and processing graphics --known as GPUs-- has dramatically improved. . In 2007 NVidia realized the potential of parallel GPU computing outside the videogame world, and released the first version of the CUDA framework, allowing software developers to use GPUs for general purpose processing. . Graphics operations are mostly linear algebra operations, and accelerating them can turn very useful in many other fields. . In 2012 Hinton et al. demonstrated the huge potential of GPUs in training deep neural networks, starting de facto the glorious days of deep learning. . from IPython.display import YouTubeVideo, HTML, display display(YouTubeVideo(&#39;-P28LKWTzrI?t=14&#39;)) . torch.cuda.is_available() . True . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) device . device(type=&#39;cuda&#39;) . t = torch.rand((3,3)) t = t.to(device) # Pay attention to re-assign the variable! t . tensor([[0.1685, 0.3253, 0.8423], [0.9466, 0.5336, 0.0559], [0.6485, 0.1583, 0.8153]], device=&#39;cuda:0&#39;) . t = torch.ones((5, 5), device=&#39;cuda&#39;) t . tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]], device=&#39;cuda:0&#39;) . t = torch.rand((3,3)) # Other methods to transfer tensors between devices # Be careful of hardcoded cuda calls: the code will not run if a GPU is not available t = t.cuda() t . tensor([[0.3631, 0.5573, 0.5631], [0.5427, 0.9746, 0.7777], [0.8968, 0.3209, 0.2966]], device=&#39;cuda:0&#39;) . t = t.cpu() t . tensor([[0.3631, 0.5573, 0.5631], [0.5427, 0.9746, 0.7777], [0.8968, 0.3209, 0.2966]]) . from typing import Union, Sequence def print_arr( *arr: Sequence[Union[torch.Tensor, np.ndarray]], prefix: str = &quot;&quot; ) -&gt; None: &quot;&quot;&quot;Pretty print tensors, together with their shape and type :param arr: one or more tensors :param prefix: prefix to use when printing the tensors &quot;&quot;&quot; print( &quot; n n&quot;.join( f&quot;{prefix}{str(x)} &lt;shape: {x.shape}&gt; &lt;dtype: {x.dtype}&gt;&quot; for x in arr ) ) . &#128214; Tensor rank . In Numpy and PyTorch the rank means number of dimensions (different meaning in Linear Algebra!) . rank-0 tensors are just scalars | . t0 = torch.tensor(3, dtype=torch.double) print_arr(t0) . tensor(3., dtype=torch.float64) &lt;shape: torch.Size([])&gt; &lt;dtype: torch.float64&gt; . item = t0.item() # convert tensor scalar to a python base type item, type(item) . (3.0, float) . try: x = torch.ones(3).item() except ValueError as e: print(&#39;Error:&#39;, e) . Error: only one element tensors can be converted to Python scalars . rank-1 tensors of length n have a shape of (n,) | . t1 = torch.tensor([1, 2, 3]) print_arr(t1) . tensor([1, 2, 3]) &lt;shape: torch.Size([3])&gt; &lt;dtype: torch.int64&gt; . print_arr(torch.tensor([42])) . tensor([42]) &lt;shape: torch.Size([1])&gt; &lt;dtype: torch.int64&gt; . # it can &quot;converted&quot; in a rank-0 tensor the .item() will work # This operation is called broadcasting, we will see it in detail in the next # lab. torch.tensor([42]).item() . 42 . rank-2 tensors have a shape of (n, m) | . t2 = torch.tensor([[1, 2, 3], [4, 5, 6]]) print_arr(t2) . tensor([[1, 2, 3], [4, 5, 6]]) &lt;shape: torch.Size([2, 3])&gt; &lt;dtype: torch.int64&gt; . t_col = t1.reshape(-1, 1) print_arr(t_col) . tensor([[1], [2], [3]]) &lt;shape: torch.Size([3, 1])&gt; &lt;dtype: torch.int64&gt; . t_row = t1.reshape(1, -1) print_arr(t_row) . tensor([[1, 2, 3]]) &lt;shape: torch.Size([1, 3])&gt; &lt;dtype: torch.int64&gt; . NOTE . Rank-1 tensors can be both row and column tensors . _ = torch.ones(10) @ torch.ones(10, 3) # This matrix multiplication should work only if `torch.ones(10)` is a column _ = torch.ones(3, 10) @ torch.ones(10) . rank-k tensors have a shape of $(n_1, dots, n_k)$ | . print_arr(torch.zeros((2, 3, 4))) . tensor([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], [[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) &lt;shape: torch.Size([2, 3, 4])&gt; &lt;dtype: torch.float32&gt; . print_arr(torch.ones((2, 2, 2, 2))) . tensor([[[[1., 1.], [1., 1.]], [[1., 1.], [1., 1.]]], [[[1., 1.], [1., 1.]], [[1., 1.], [1., 1.]]]]) &lt;shape: torch.Size([2, 2, 2, 2])&gt; &lt;dtype: torch.float32&gt; . EXERCISE . Build a tensor $X in mathbb{R}^{k times k}$ filled with zeros and the sequence $[0, ..., k-1]$ along the diagonal . k = 3 . torch.tensor([[0,0,0], [0,1,0], [0,0,2]]) . tensor([[0, 0, 0], [0, 1, 0], [0, 0, 2]]) . EXERCISE . What is the shape of the following tensor? . torch.tensor( [ [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], [[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], ] ) . # the shape of this tensor. . Changing and adding dimensions . PyTorch provides several functions to manipulate tensor shapes . Transpose dimension . a = torch.ones((3, 5)) a[0, -1] = 0 print(&quot;a: &quot;) print_arr(a) . a: tensor([[1., 1., 1., 1., 0.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) &lt;shape: torch.Size([3, 5])&gt; &lt;dtype: torch.float32&gt; . a.T . tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [0., 1., 1.]]) . a.transpose(1, 0) # Swap dimension 1 and 0 . tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [0., 1., 1.]]) . torch.einsum(&#39;ij -&gt; ji&#39;, a) # transpose using Einstein notation # In the next notebook we will explain the Einstein notation in detail . tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [0., 1., 1.]]) . &#128214; Transpose in in k-dimensions and in numpy? . a = torch.ones((2, 3, 6)) a[1, 2, 4] = 42 print_arr(a) . tensor([[[ 1., 1., 1., 1., 1., 1.], [ 1., 1., 1., 1., 1., 1.], [ 1., 1., 1., 1., 1., 1.]], [[ 1., 1., 1., 1., 1., 1.], [ 1., 1., 1., 1., 1., 1.], [ 1., 1., 1., 1., 42., 1.]]]) &lt;shape: torch.Size([2, 3, 6])&gt; &lt;dtype: torch.float32&gt; . a.transpose(2, 1) . tensor([[[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 42.], [ 1., 1., 1.]]]) . torch.einsum(&#39;ijk-&gt;ikj&#39;, a) . tensor([[[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]], [[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 42.], [ 1., 1., 1.]]]) . # Most of the time readability is the most important goal to aim for # What do you think a.T will do with the three dimensional tensor? (without looking at the docs!) # a.T # Spoiler: it is confusing even after reading the docs for tensor with shape &gt;2 # Prefer readable code to short code! . NOTE . In Numpy the transpose function is different! . PyTorch:&gt; torch.transpose(input, dim0, dim1) → Tensor&gt; &gt; NumPy:&gt; numpy.transpose(a, axes=None) -&gt; numpy.ndarray&gt; &gt; Compare the docs from numpy and pytorch&gt; In PyTorch the transpose swaps two dimensions. In NumPy you can specify an entire mapping to shange all the dimensions. . a = np.arange(10).reshape(2, 5) a . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . a.transpose(1, 0) . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . a.transpose(0, 1) . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . torch.from_numpy(a).transpose(0, 1) . tensor([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . # pretty much everywhere: PyTorch, NumPy, TensorFlow, Jax, ... # We will see the power of einsum in the next lab np.einsum(&#39;ij -&gt; ji&#39;, a) . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . Reshape . Another important feature is reshaping a tensor into different dimensions . We need to make sure to preserve the same number of elements. | -1 in one of the dimensions means &quot;figure it out&quot;. | . ❌❌❌ Pay attention that transposing and reshaping are two fundamentally different operations: . a = torch.arange(12).reshape(3,4 ) a . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . a.t() . tensor([[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]]) . a.reshape(4, 3) . tensor([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]]) . &#128214; What reshape is really doing? . Think of the reshape operation as unrolling the tensor row-wise, to obtain a rank-1 tensor (matlab users: matlab unrolls column-wise, pay attention when converting code!). Then organize the values in this tensor following the specified dimensions. . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . $-$ unrolling $ to $ . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . Then, reading the target shape from right to left, organize the values into the dimensions: . e.g. reshape into [4, 3]: | . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . $-$ organize in groups of $3$ $ to $ . tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]) . $-$ organize in groups of $4$ $ to $ . tensor([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]]) # same shape of corresponding transpose but it **is** different . e.g. reshape into [2, 2, 3]: | . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . $-$ organize in groups of $3$ $ to $ . tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]) . $-$ organize in groups of $2$ $ to $ . tensor([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]]) . $-$ organize in groups of $2$ $ to $ . tensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) . a = torch.arange(12) print_arr(a) . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) &lt;shape: torch.Size([12])&gt; &lt;dtype: torch.int64&gt; . a.reshape(6, 2) . tensor([[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9], [10, 11]]) . a.reshape(2, 6) . tensor([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]]) . a.reshape(2, 2, 3) . tensor([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) . try: a.reshape(5, -1) except RuntimeError as e: print(&#39;Error:&#39;, e) . Error: shape &#39;[5, -1]&#39; is invalid for input of size 12 . a.reshape(1, -1) . tensor([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]) . a.reshape(-1, 1) . tensor([[ 0], [ 1], [ 2], [ 3], [ 4], [ 5], [ 6], [ 7], [ 8], [ 9], [10], [11]]) . a.reshape(-1) # we are flattening the rank-k tensor into a rank-1 tensor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . NOTE . We can add or remove dimensions of size 1 using torch.unsqueeze or torch.squeeze . a . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . a.unsqueeze(0).shape . torch.Size([1, 12]) . a.unsqueeze(-1).shape . torch.Size([12, 1]) . NOTE . Often the reshape does not require a physical copy of the data, but just a logical reorganization. . If you are curious about the NumPy/PyTorch tensor internals, a good starting point to learn about strides is this SO answer.&gt; tldr:often you can reshape tensors by changing only its strides and shape. The strides are the byte-separation between consecutive items for each dimension.&gt; &gt; To be sure to obtain a view of the tensor, that shares the same underlying data, you can use the torch.view method. Its semantics is similar to reshape, but it works only on contiguous tensors and it guarantees that no copy will be performed. . EXERCISE . Given a rank-1 array of increasing numbers from 0 to 9, defined as:&gt;&gt; python a = torch.arange(10) . Use only the `reshape` and `transpose` functions to obtain the following tensor from `a`:&gt;&gt; python tensor([0, 2, 4, 6, 8, 1, 3, 5, 7, 9]) . a = torch.arange(10) print(a) . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . Concatenation . PyTorch provides many functions to manipulate tensors. Two of the most common functions are: . torch.stack: Concatenates a sequence of tensors along a new dimension. | torch.cat: Concatenates a sequence of tensors in the given dimension. | . a = torch.arange(12).reshape(3, 4) b = torch.arange(12).reshape(3, 4) + 100 print_arr(a, b) . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) &lt;shape: torch.Size([3, 4])&gt; &lt;dtype: torch.int64&gt; tensor([[100, 101, 102, 103], [104, 105, 106, 107], [108, 109, 110, 111]]) &lt;shape: torch.Size([3, 4])&gt; &lt;dtype: torch.int64&gt; . out = torch.cat((a, b), dim=0) print_arr(out) . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [100, 101, 102, 103], [104, 105, 106, 107], [108, 109, 110, 111]]) &lt;shape: torch.Size([6, 4])&gt; &lt;dtype: torch.int64&gt; . out = torch.cat((a, b), dim=1) print_arr(out) . tensor([[ 0, 1, 2, 3, 100, 101, 102, 103], [ 4, 5, 6, 7, 104, 105, 106, 107], [ 8, 9, 10, 11, 108, 109, 110, 111]]) &lt;shape: torch.Size([3, 8])&gt; &lt;dtype: torch.int64&gt; . out = torch.stack((a, b), dim=0) print_arr(out) . tensor([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[100, 101, 102, 103], [104, 105, 106, 107], [108, 109, 110, 111]]]) &lt;shape: torch.Size([2, 3, 4])&gt; &lt;dtype: torch.int64&gt; . EXERCISE . Given a tensor $X in mathbb{R}^{3 times 1920 times 5 times 1080}$ reorganize it in order to obtain a tensor $Y in mathbb{R}^{5 times 1920 times 1080 times 3}$ . Think of $X$ as a tensor that represents $5$ RGB images, each one with width $1920$ and height $1080$. Your goal is to reorganize this tensor in a sensible (and usable) way. . a = torch.rand(3, 1920, 5, 1080) a.shape . torch.Size([3, 1920, 5, 1080]) . . a.transpose(0, 2).transpose(2,3).shape # Equivalent solution a.permute(2, 1, 3, 0).shape . Tensor indexing . PyTorch offers several ways to index tensors . Standard indexing . As a standard Python list, PyTorch tensors support the python indexing conventions: . a = torch.arange(10) a . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . print(a[0]) # first element print(a[1]) # second element . tensor(0) tensor(1) . print(a[-1]) # last element print(a[-2]) # second last element . tensor(9) tensor(8) . Multidimensional indexing . Since tensors may be multidimensional, you can specify one index for each dimension: . a = torch.arange(10).reshape(2, 5) a . tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . a[1, 3] . tensor(8) . a[0] . tensor([0, 1, 2, 3, 4]) . a[1] . tensor([5, 6, 7, 8, 9]) . a[0, -1] . tensor(4) . EXERCISE . Which is the element in a[1, -1]? . Slicing . Similar to Python sequences and Numpy arrays, PyTorch tensors can be easily sliced using the slice notation: . a[start:stop] # items from start to stop-1 (i.e. the last element is excluded) a[start:] # items from start through the rest of the array a[:stop] # items from the beginning through stop-1 a[:] # a copy of the whole array . There is also an optional step value, which can be used with any of the above: . a[start:stop:step] # from start to at most stop-1, by step . a = torch.arange(10) + 10 a . tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) . a[5:7] . tensor([15, 16]) . a[-5:] . tensor([15, 16, 17, 18, 19]) . a[::2] . tensor([10, 12, 14, 16, 18]) . With multidimensional tensors we can perform multidimensional slicing: . a = torch.arange(10).reshape(2, 5) a . tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . a[:, 1] . tensor([1, 6]) . a[:, -1] . tensor([4, 9]) . a[-1, -3:] . tensor([7, 8, 9]) . You can assign to sliced tensors, therefore modifying the original tensor. . Indexing and slicing operations do their best to return tensors that share the underlying data with the original tensor. . a = torch.arange(10).reshape(2, 5) a . tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . b = a[0:2, 1:3] b . tensor([[1, 2], [6, 7]]) . b[-1, :] = -999 b . tensor([[ 1, 2], [-999, -999]]) . a . tensor([[ 0, 1, 2, 3, 4], [ 5, -999, -999, 8, 9]]) . a[-1, -1] = -1 a . tensor([[ 0, 1, 2, 3, 4], [ 5, -999, -999, 8, -1]]) . NOTE . Indexing with integers yields lower rank tensors . a = torch.arange(12).reshape(3, 4) print_arr(a) . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) &lt;shape: torch.Size([3, 4])&gt; &lt;dtype: torch.int64&gt; . row_r1 = a[1, :] print_arr(row_r1) . tensor([4, 5, 6, 7]) &lt;shape: torch.Size([4])&gt; &lt;dtype: torch.int64&gt; . row_r2 = a[1:2, :] print_arr(row_r2) . tensor([[4, 5, 6, 7]]) &lt;shape: torch.Size([1, 4])&gt; &lt;dtype: torch.int64&gt; . row_r3 = a[[1], :] print_arr(row_r3) . tensor([[4, 5, 6, 7]]) &lt;shape: torch.Size([1, 4])&gt; &lt;dtype: torch.int64&gt; . print_arr(a[:, 1]) print_arr(a[:, [1]]) . tensor([1, 5, 9]) &lt;shape: torch.Size([3])&gt; &lt;dtype: torch.int64&gt; tensor([[1], [5], [9]]) &lt;shape: torch.Size([3, 1])&gt; &lt;dtype: torch.int64&gt; . &#128214;&#128214; Slice Object . The slice syntax is just a shortand. . In Python everything is an object, even a slice. It is possible to explicitly create a Slice object and reuse it to index multiple tensors in the same way: . s1 = slice(3) # equivalent to the slice [:3] s1 . slice(None, 3, None) . type(s1) # It is a built-in type . slice . out = a[s1] # equivalent to a[:3] print_arr(a, out) . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) &lt;shape: torch.Size([3, 4])&gt; &lt;dtype: torch.int64&gt; tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) &lt;shape: torch.Size([3, 4])&gt; &lt;dtype: torch.int64&gt; . mystring = &#39;this is just a string&#39; mystring[s1] . &#39;thi&#39; . s2 = slice(None, None, -1) mystring[s2] . &#39;gnirts a tsuj si siht&#39; . try: a[s2] # PyTorch currently does not support negative steps except ValueError as e: print(&#39;Error:&#39;, e) . Error: step must be greater than zero . &#128214; Integer array indexing . It is a generalization of the slice: when you slice, the resulting tensor view will always be a subarray of the original tensor. . Integer array indexing allows you to construct arbitrary tensors, using the data from another tensor . a = torch.arange(1, 7).reshape(3, 2) print_arr(a) . tensor([[1, 2], [3, 4], [5, 6]]) &lt;shape: torch.Size([3, 2])&gt; &lt;dtype: torch.int64&gt; . # The returned array will have shape (3,) b = a[[0, 1, 2], [0, 1, 0]] print_arr(b) . tensor([1, 4, 5]) &lt;shape: torch.Size([3])&gt; &lt;dtype: torch.int64&gt; . v1, v2, v3 = a[0, 0], a[1, 1], a[2, 0] b = torch.tensor([v1, v2, v3]) print_arr(b) . tensor([1, 4, 5]) &lt;shape: torch.Size([3])&gt; &lt;dtype: torch.int64&gt; . print_arr(a[[0, 0], [1, 1]]) print_arr(torch.tensor([a[0, 1], a[0, 1]])) . tensor([2, 2]) &lt;shape: torch.Size([2])&gt; &lt;dtype: torch.int64&gt; tensor([2, 2]) &lt;shape: torch.Size([2])&gt; &lt;dtype: torch.int64&gt; . # as long as they have dtype=torch.int64 (synonym for torch.long) i = torch.ones(3, dtype=torch.int64) i . tensor([1, 1, 1]) . j = torch.tensor([0, 1, 0]) j . tensor([0, 1, 0]) . out = a[i, j] print_arr(a, out) . tensor([[1, 2], [3, 4], [5, 6]]) &lt;shape: torch.Size([3, 2])&gt; &lt;dtype: torch.int64&gt; tensor([3, 4, 3]) &lt;shape: torch.Size([3])&gt; &lt;dtype: torch.int64&gt; . EXERCISE . Change with a single assignment the elements in the tensor $X in mathbb{R}^{4 times 3}$ as follows:&gt;&gt; X[0,2] = -1 . X[1,1] = 0 . X[2,0] = 1 . X[3,1] = 2 . a = torch.arange(12).reshape(4, 3) a . tensor([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11]]) . ❌❌❌ NOTE . Slice indexing vs Array indexing . Be careful that slice indexing and array indexing are different operations! . a = torch.arange(16).reshape(4, 4) a . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) . a[0:3, 0:3] . tensor([[ 0, 1, 2], [ 4, 5, 6], [ 8, 9, 10]]) . a[[0, 1, 2], [0, 1, 2]] . tensor([ 0, 5, 10]) . a[torch.arange(0,3), torch.arange(0,3)] . tensor([ 0, 5, 10]) . a[0:5:2, 0:5:2] . tensor([[ 0, 2], [ 8, 10]]) . . Boolean array indexing . This type of indexing is used to select the elements of a tensor that satisfy some condition (similar to MATLAB&#39;s logical indexing): . a = torch.arange(6).reshape(3, 2) a . tensor([[0, 1], [2, 3], [4, 5]]) . bool_idx = (a &gt; 2) bool_idx . tensor([[False, False], [False, True], [ True, True]]) . a[bool_idx] # remember that NumPy and PyTorch unroll row-wise and not column-wise like Matlab . tensor([3, 4, 5]) . If you want to know more about indexing in PyTorch and Numpy read the documentation . &#128214;&#128214; Graph use case . Assume to have a weighted adjacency matrix for a non-directed graph possibly with self-loops. We want to obtain the list of edges of this graph for which the weights is greater than 0.5. . How can we do that? . a = torch.randint(2, (5, 5)).bool() adj_matrix = ((a + a.T) &gt; 0.5) * torch.rand_like(a, dtype=torch.float) adj_matrix . tensor([[0.0000, 0.8826, 0.0000, 0.0000, 0.5513], [0.4905, 0.0000, 0.5016, 0.1186, 0.4184], [0.0000, 0.0022, 0.0000, 0.8533, 0.4365], [0.0000, 0.5070, 0.6087, 0.0753, 0.1995], [0.2450, 0.6027, 0.2909, 0.9022, 0.0000]]) . (adj_matrix &gt; 0.5).nonzero() . tensor([[0, 1], [0, 4], [1, 2], [2, 3], [3, 1], [3, 2], [4, 1], [4, 3]]) . adj_matrix[adj_matrix &gt; 0.5] . tensor([0.8826, 0.5513, 0.5016, 0.8533, 0.5070, 0.6087, 0.6027, 0.9022]) . Exercises . EXERCISE . Build a 3D tensor in $X in mathbb{R}^{3 times 3 times 3}$ that has ones along the 3D-diagonal and zeros elsewhere, i.e. a 3D identity. . . EXERCISE . Given a 3D tensor $X in mathbb{R}^{w times h times 3}$ representing a $w times h$ image with (r, g, b) color channels. Assume the color channel is the last dimension in the tensor and each color component $c in [0, 1]$. . Color the image $X$ completely by red, i.e. (1, 0, 0) in the (r, g, b) format. . x = torch.zeros(100, 200, 3) %matplotlib inline import matplotlib.pyplot as plt img = plt.imshow(x) . # y = ? # img = plt.imshow(y) . EXERCISE . Given the GitHub logo $X in mathbb{R}^{560 times 560}$. Assume the logo is in gray scale, with the color $c in [0, 1]$ (remember 0 $ to$ black). . Change the black-ish color into light gray:$0.8$.&gt; 2. Then draw a diagonal and anti-diagonal black line (i.e. an X) on the new image, to mark that the new logo is wrong. | from skimage import io image = io.imread(&#39;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&#39;, as_gray=True) _ = plt.imshow(image, cmap=&#39;gray&#39;, vmin=0, vmax=1) . X = torch.from_numpy(image.copy()) # PyTorch CPU and Numpy share the memory! _ = plt.imshow(X, cmap=&#39;gray&#39;, vmin=0, vmax=1) . # # ? _ = plt.imshow(X, cmap=&#39;gray&#39;, vmin=0, vmax=1) .",
            "url": "noranta4.com/2023/03/06/_Tensor_basics_2023.html",
            "relUrl": "/2023/03/06/_Tensor_basics_2023.html",
            "date": " • Mar 6, 2023"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "",
          "content": ". I started my PhD in AI at the Sapienza University of Rome, CS department in late 2019. I work in the GLADIA research group advised by Emanuele Rodolà. . Before joining the group, my studies have embraced two big fields, physics and computer science, and have been contaminated by many others, thanks to the SSAS interdisciplinary honor program. . . What makes us humans? My personal search for an answer today is focused on the fundamental nature of knowledge and what makes learning possible. . My current research interests include two great challenges of deep learning, namely the data/computation bottlenecks and the generalization gaps. Today my best bet to solve these challenges involves including natural language in the neural pipeline. . . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "noranta4.com/_pages/aboutOLD.html",
          "relUrl": "/_pages/aboutOLD.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "noranta4.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}